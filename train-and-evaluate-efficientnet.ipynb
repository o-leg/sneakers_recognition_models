{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8375658,"sourceType":"datasetVersion","datasetId":4980008},{"sourceId":8375811,"sourceType":"datasetVersion","datasetId":4980137}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train and Evaluate EfficientNet\n\n\n***This notebook serves as debuging file for model loading as at some point of time we encountered that there was an issue with the way saved|loaded weights.***","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport plotly.graph_objs as go\n\nfrom keras.models import load_model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.applications import MobileNetV2, MobileNet, MobileNetV3Small, EfficientNetB0, EfficientNetB1, EfficientNetB3, EfficientNetB4, EfficientNetB7\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc\n\nimport plotly.io as pio","metadata":{"id":"BTvqUlqRnpXr","execution":{"iopub.status.busy":"2024-05-09T14:31:37.433256Z","iopub.execute_input":"2024-05-09T14:31:37.433614Z","iopub.status.idle":"2024-05-09T14:31:50.421673Z","shell.execute_reply.started":"2024-05-09T14:31:37.433585Z","shell.execute_reply":"2024-05-09T14:31:50.420648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setup pipeline veriables","metadata":{"id":"UwzVnTfMrV3e","execution":{"iopub.status.busy":"2024-03-06T13:21:29.243655Z","iopub.execute_input":"2024-03-06T13:21:29.244264Z","iopub.status.idle":"2024-03-06T13:21:29.250988Z","shell.execute_reply.started":"2024-03-06T13:21:29.244231Z","shell.execute_reply":"2024-03-06T13:21:29.249871Z"}}},{"cell_type":"code","source":"SEED = 20\nDRIVE_PATH = \"/kaggle\"\nDATA_DIRECTORY = f\"{DRIVE_PATH}/input/sneakers-recognition-dataset-v22/Sneakers_Recognition_DataSet\"\nSHOW_DATASET_STRUCTURE = True\nSHOW_IMAGE_SHAPE_MEANS = True\nShOW_IMAGE_SHAPE_PLOT = True\nDRAW_CLASS_DISTRIBUTION = True\n\n# IMAGE_SIZE = (300, 300)\n\n# IMAGE_SIZE = (244, 244)\n\n# IMAGE_SIZE = (224, 224)\nIMAGE_SIZE = (240, 240)\nBATCH_SIZE = 32\nEPOCHS = 13\nNUMBER_LAYERS_TO_FREEZE = 0 \nMODEL_CLASS_CALLBACK = EfficientNetB1\nSHOW_NUMBER_OF_LAYERS_IN_BASE_MODEL = True\nSHOW_MODEL_SUMMARY = False\n\nMODEL_SAVING_PATH = f\"{DRIVE_PATH}/working/models/DATA_V2_{MODEL_CLASS_CALLBACK}_{NUMBER_LAYERS_TO_FREEZE}Frozen_{BATCH_SIZE}Batch_{IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}Size_AugV2\"","metadata":{"execution":{"iopub.status.busy":"2024-05-09T14:31:50.423501Z","iopub.execute_input":"2024-05-09T14:31:50.424052Z","iopub.status.idle":"2024-05-09T14:31:50.430209Z","shell.execute_reply.started":"2024-05-09T14:31:50.424025Z","shell.execute_reply":"2024-05-09T14:31:50.429305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.random import seed \nseed(SEED) # keras seed fixing import\n\nimport tensorflow\ntensorflow.random.set_seed(SEED) # tensorflow seed fixing","metadata":{"execution":{"iopub.status.busy":"2024-05-09T14:31:50.431256Z","iopub.execute_input":"2024-05-09T14:31:50.431540Z","iopub.status.idle":"2024-05-09T14:31:50.455851Z","shell.execute_reply.started":"2024-05-09T14:31:50.431518Z","shell.execute_reply":"2024-05-09T14:31:50.454840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data insights","metadata":{}},{"cell_type":"code","source":"def list_directories(root_dir):\n    \"\"\"\n    Recursively lists all directories and subdirectories within the specified directory.\n\n    Args:\n    - root_dir (str): The directory to list.\n\n    Returns:\n    - directories (list): A list of directory paths.\n    \"\"\"\n    directories = []\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        directories.append(dirpath)\n        for dirname in dirnames:\n            directories.append(os.path.join(dirpath, dirname))\n    return directories\n\n\nif SHOW_DATASET_STRUCTURE:\n    directories_structure = list_directories(DATA_DIRECTORY)\n    for directory in directories_structure:\n        print(directory)\n","metadata":{"id":"P5iOB85MiSHc","execution":{"iopub.status.busy":"2024-05-09T14:31:50.459968Z","iopub.execute_input":"2024-05-09T14:31:50.460252Z","iopub.status.idle":"2024-05-09T14:31:51.703227Z","shell.execute_reply.started":"2024-05-09T14:31:50.460229Z","shell.execute_reply":"2024-05-09T14:31:51.702216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate mean dimensions of images in a directory\ndef calculate_mean_dimensions(directory):\n    total_width = 0\n    total_height = 0\n    total_images = 0\n\n    # Iterate through files in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"): # Add other formats if needed\n            img_path = os.path.join(directory, filename)\n            img = cv2.imread(img_path)\n            if img is not None:\n                total_width += img.shape[1]\n                total_height += img.shape[0]\n                total_images += 1\n\n    # Calculate mean dimensions\n    if total_images > 0:\n        mean_width = total_width / total_images\n        mean_height = total_height / total_images\n        return mean_width, mean_height\n    else:\n        return 0, 0\n\nif SHOW_IMAGE_SHAPE_MEANS:\n    # Iterate through subdirectories\n    for subdir in os.listdir(DATA_DIRECTORY):\n        subdir_path = os.path.join(DATA_DIRECTORY, subdir)\n        if os.path.isdir(subdir_path):\n            mean_width, mean_height = calculate_mean_dimensions(subdir_path)\n            print(f\"Directory: {subdir}, Mean Width: {mean_width}, Mean Height: {mean_height}\")\n","metadata":{"id":"zoJ-mrYM9sLN","execution":{"iopub.status.busy":"2024-05-09T14:31:51.704388Z","iopub.execute_input":"2024-05-09T14:31:51.704680Z","iopub.status.idle":"2024-05-09T14:34:10.566228Z","shell.execute_reply.started":"2024-05-09T14:31:51.704656Z","shell.execute_reply":"2024-05-09T14:34:10.565259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collect_dimensions(directory):\n    widths = []\n    heights = []\n\n    # Iterate through files in the directory\n    for root, _, files in os.walk(directory):\n        print(f\"Gathering data from {root} directory\")\n        for filename in files:\n            if filename.endswith(\".jpg\") or filename.endswith(\".png\"): # Add other formats if needed\n                img_path = os.path.join(root, filename)\n                img = cv2.imread(img_path)\n                if img is not None:\n                    widths.append(img.shape[1])\n                    heights.append(img.shape[0])\n\n    return widths, heights\n\ndef plot_dimension_distribution(widths, heights):\n    # Create histograms\n    width_hist = go.Histogram(x=widths, name='Width', marker_color='blue')\n    height_hist = go.Histogram(x=heights, name='Height', marker_color='green')\n\n    # Create figure\n    fig = go.Figure(data=[width_hist, height_hist])\n\n    # Update layout\n    fig.update_layout(\n        title='Image Dimension Distribution',\n        xaxis=dict(title='Dimension'),\n        yaxis=dict(title='Frequency'),\n        barmode='overlay',\n        bargap=0.1\n    )\n\n    # Show plot\n    fig.show()\n\ndef visualize_distribution(directory):\n    widths, heights = collect_dimensions(directory)\n    plot_dimension_distribution(widths, heights)\n\n\nif ShOW_IMAGE_SHAPE_PLOT:\n  # Visualize distribution\n  visualize_distribution(DATA_DIRECTORY)\n","metadata":{"id":"XukpOdIfGcHu","execution":{"iopub.status.busy":"2024-05-09T14:34:10.567312Z","iopub.execute_input":"2024-05-09T14:34:10.567649Z","iopub.status.idle":"2024-05-09T14:36:01.926537Z","shell.execute_reply.started":"2024-05-09T14:34:10.567623Z","shell.execute_reply":"2024-05-09T14:36:01.925414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create train and validation data generators\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    rotation_range = 30,\n    shear_range = 0.2,\n    height_shift_range = 0.2,\n    width_shift_range = 0.2,\n    brightness_range = (0.2, 1.0),\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    fill_mode = 'nearest'\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    DATA_DIRECTORY,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='training',  # specify this as training data\n    seed=SEED,\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    DATA_DIRECTORY,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='validation',  # specify this as validation data\n    seed=SEED,\n)\n\nall_labels = set(train_generator.labels).union(validation_generator.labels)\nnumber_of_labels = len(all_labels)","metadata":{"id":"qllRXxP9i0Lk","outputId":"2b7d44b4-11d1-4de8-f6d9-562d1061edf2","execution":{"iopub.status.busy":"2024-05-09T14:36:01.927767Z","iopub.execute_input":"2024-05-09T14:36:01.928102Z","iopub.status.idle":"2024-05-09T14:36:02.149586Z","shell.execute_reply.started":"2024-05-09T14:36:01.928066Z","shell.execute_reply":"2024-05-09T14:36:02.148507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_generator.class_indices","metadata":{"execution":{"iopub.status.busy":"2024-05-09T14:36:02.150894Z","iopub.execute_input":"2024-05-09T14:36:02.151673Z","iopub.status.idle":"2024-05-09T14:36:02.160221Z","shell.execute_reply.started":"2024-05-09T14:36:02.151634Z","shell.execute_reply":"2024-05-09T14:36:02.159340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_class_distribution_bar_plot(class_distribution, save_path):\n    sorted_class_distribution = dict(sorted(class_distribution.items()))\n    fig = go.Figure(data=[go.Bar(x=list(sorted_class_distribution.keys()), y=list(sorted_class_distribution.values()))])\n    fig.update_layout(title='Class Distribution',\n                      xaxis_title='Class',\n                      yaxis_title='Count',\n                      width=1000,\n                      height=600)\n    fig.write_html(save_path)  # Save the plot to the specified path as HTML\n    fig.show()\n\n# Example usage of drawing class distribution bar plot\ndef draw_class_distribution(generator, save_path):\n    class_indices_mapping = generator.class_indices\n\n    # Reverse the mapping to get class names from indices\n    class_names = {v: k for k, v in class_indices_mapping.items()}\n    \n    class_distribution = {}\n    for label in generator.labels:\n        if label in class_distribution:\n            class_distribution[label] += 1\n        else:\n            class_distribution[label] = 1\n\n    draw_class_distribution_bar_plot(class_distribution, save_path)\n    \n\nif DRAW_CLASS_DISTRIBUTION:\n    draw_class_distribution(train_generator, os.path.join(f\"{DRIVE_PATH}/working\", \"class_distribution_train.html\"))\n    draw_class_distribution(validation_generator, os.path.join(f\"{DRIVE_PATH}/working\", \"class_distribution_validation.html\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T14:36:02.161518Z","iopub.execute_input":"2024-05-09T14:36:02.162497Z","iopub.status.idle":"2024-05-09T14:36:02.247216Z","shell.execute_reply.started":"2024-05-09T14:36:02.162461Z","shell.execute_reply":"2024-05-09T14:36:02.246222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"# Define the input layer with your desired input shape\ninput_layer = Input(shape=(*IMAGE_SIZE, 3))\n\n# Load the EfficientNetB0 model with pre-trained weights, excluding the top layer\nbase_model = MODEL_CLASS_CALLBACK(input_tensor=input_layer, weights='imagenet', include_top=False)","metadata":{"id":"tfHPn1uizipg","outputId":"9b931d06-149e-4702-e5d3-b16965609a74","execution":{"iopub.status.busy":"2024-05-09T14:36:02.251648Z","iopub.execute_input":"2024-05-09T14:36:02.251942Z","iopub.status.idle":"2024-05-09T14:36:07.479683Z","shell.execute_reply.started":"2024-05-09T14:36:02.251917Z","shell.execute_reply":"2024-05-09T14:36:07.478819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the number of trainable and non-trainable layers\ndef count_trainable_layers(model):\n    trainable_layers = 0\n    non_trainable_layers = 0\n\n    for layer in model.layers:\n        if layer.trainable:\n            trainable_layers += 1\n        else:\n            non_trainable_layers += 1\n\n    return trainable_layers, non_trainable_layers\n\nif SHOW_NUMBER_OF_LAYERS_IN_BASE_MODEL:\n    # Get the number of trainable and non-trainable layers\n    trainable_layers, non_trainable_layers = count_trainable_layers(base_model)\n\n    print(\"Number of trainable layers:\", trainable_layers)\n    print(\"Number of non-trainable layers:\", non_trainable_layers)\n","metadata":{"id":"jCEH6wRZzgaK","outputId":"8815431d-7305-44d6-d50e-859b3c82c92a","execution":{"iopub.status.busy":"2024-05-09T14:36:07.481408Z","iopub.execute_input":"2024-05-09T14:36:07.481759Z","iopub.status.idle":"2024-05-09T14:36:07.489769Z","shell.execute_reply.started":"2024-05-09T14:36:07.481728Z","shell.execute_reply":"2024-05-09T14:36:07.488535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SHOW_MODEL_SUMMARY:\n    base_model.summary()","metadata":{"id":"2B9Av6KBvehm","outputId":"e5917b6a-3fb3-4e4c-e3b1-c14a1bee2a33","execution":{"iopub.status.busy":"2024-05-09T14:36:07.491081Z","iopub.execute_input":"2024-05-09T14:36:07.491442Z","iopub.status.idle":"2024-05-09T14:36:07.509057Z","shell.execute_reply.started":"2024-05-09T14:36:07.491408Z","shell.execute_reply":"2024-05-09T14:36:07.508178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number_of_labels = 33","metadata":{"execution":{"iopub.status.busy":"2024-05-09T14:36:07.510203Z","iopub.execute_input":"2024-05-09T14:36:07.510508Z","iopub.status.idle":"2024-05-09T14:36:07.518347Z","shell.execute_reply.started":"2024-05-09T14:36:07.510484Z","shell.execute_reply":"2024-05-09T14:36:07.517532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    base_model = MODEL_CLASS_CALLBACK(input_tensor=input_layer, weights='imagenet', include_top=False)\n    # # Freeze layers\n    for layer in base_model.layers[:NUMBER_LAYERS_TO_FREEZE]:\n        layer.trainable = False\n    \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    predictions = Dense(number_of_labels, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model\n\nmodel = create_model()\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-05-09T14:36:07.519496Z","iopub.execute_input":"2024-05-09T14:36:07.519822Z","iopub.status.idle":"2024-05-09T14:36:09.077719Z","shell.execute_reply.started":"2024-05-09T14:36:07.519791Z","shell.execute_reply":"2024-05-09T14:36:09.076608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import LearningRateScheduler\n\n# Define the learning rate schedule function\ndef lr_schedule(epoch):\n    initial_lr = 0.001\n    if epoch is set([15, 25]):\n        return initial_lr * 0.1\n    return initial_lr\n\n# Define the LearningRateScheduler callback\nlr_scheduler = LearningRateScheduler(lr_schedule)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T14:36:09.079325Z","iopub.execute_input":"2024-05-09T14:36:09.079752Z","iopub.status.idle":"2024-05-09T14:36:09.085692Z","shell.execute_reply.started":"2024-05-09T14:36:09.079716Z","shell.execute_reply":"2024-05-09T14:36:09.084551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure the directory exists\nos.makedirs(MODEL_SAVING_PATH, exist_ok=True)\n\n# Define a callback to save the model weights after every epoch\ncheckpoint_filepath = MODEL_SAVING_PATH + \"/model_weights_epoch_{epoch:02d}.weights.h5\"\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=False,\n    verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T14:36:09.087033Z","iopub.execute_input":"2024-05-09T14:36:09.087951Z","iopub.status.idle":"2024-05-09T14:36:09.096161Z","shell.execute_reply.started":"2024-05-09T14:36:09.087922Z","shell.execute_reply":"2024-05-09T14:36:09.095224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train our model","metadata":{}},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=EPOCHS,\n    callbacks=[model_checkpoint_callback, lr_scheduler]\n)","metadata":{"id":"MJYKFVTOjXQ-","outputId":"cad35a00-161e-4ab9-b7ca-02579bd59694","execution":{"iopub.status.busy":"2024-05-09T14:36:09.097215Z","iopub.execute_input":"2024-05-09T14:36:09.097511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the trained model\nmodel.save(f\"{MODEL_SAVING_PATH}/model.h5\")","metadata":{"id":"p4wYg_srWb4p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation","metadata":{}},{"cell_type":"code","source":"# Get training and validation loss values\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Get training and validation accuracy values\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\n# Create traces for loss\nloss_trace = go.Scatter(x=list(range(1, len(train_loss) + 1)), y=train_loss, mode='lines', name='Training Loss')\nval_loss_trace = go.Scatter(x=list(range(1, len(val_loss) + 1)), y=val_loss, mode='lines', name='Validation Loss')\n\n# Create traces for accuracy\nacc_trace = go.Scatter(x=list(range(1, len(train_acc) + 1)), y=train_acc, mode='lines', name='Training Accuracy')\nval_acc_trace = go.Scatter(x=list(range(1, len(val_acc) + 1)), y=val_acc, mode='lines', name='Validation Accuracy')\n\n# Create figure for loss\nloss_fig = go.Figure(data=[loss_trace, val_loss_trace])\nloss_fig.update_layout(title='Training and Validation Loss',\n                       xaxis_title='Epoch',\n                       yaxis_title='Loss')\n\n# Create figure for accuracy\nacc_fig = go.Figure(data=[acc_trace, val_acc_trace])\nacc_fig.update_layout(title='Training and Validation Accuracy',\n                      xaxis_title='Epoch',\n                      yaxis_title='Accuracy')\n\n# Save figures as HTML files\npio.write_html(loss_fig, f\"{MODEL_SAVING_PATH}/loss_figure.html\")\npio.write_html(acc_fig, f\"{MODEL_SAVING_PATH}/accuracy_figure.html\")\n","metadata":{"id":"yNehpeTczZsl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_true_and_predicted(generator, model):\n    # Get true labels and predicted labels\n    true_labels = []\n    predicted_labels = []\n    predicted_probabilities = []\n    for i in range(len(generator)):\n        batch = generator[i]\n        true_labels.extend(np.argmax(batch[1], axis=1))  # Extract true labels from the batch\n        predictions = model.predict(batch[0])\n        predicted_labels.extend(np.argmax(predictions, axis=1))  # Extract predicted labels\n        predicted_probabilities.extend(predictions)  # Extract predicted probabilities\n    return true_labels, predicted_labels, predicted_probabilities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Function to plot confusion matrix using Plotly\ndef plot_confusion_matrix(conf_matrix, class_names, title, save_path=None):\n    trace = go.Heatmap(z=np.flipud(conf_matrix),  # Reverse the y-axis\n                       x=class_names,\n                       y=class_names[::-1],  # Reverse the y-axis labels\n                       colorscale='Blues',\n                       colorbar=dict(title='Count'),\n                       )\n    layout = go.Layout(title=title,\n                       xaxis=dict(title='Predicted labels'),\n                       yaxis=dict(title='True labels'),\n                       width=1000,  # Specify width of the plot\n                       height=1000,  # Specify height of the plot\n#                        legend=dict(title='Legend'),  # Include legend\n                       )\n    fig = go.Figure(data=[trace], layout=layout)\n    if save_path:\n        fig.write_html(save_path)  # Save the plot to the specified path as HTML\n    fig.show()\n\n# Function to calculate metrics and plot ROC-AUC curve\ndef calculate_metrics_and_plot_roc(true_labels, predicted_probabilities, class_names, title, save_path=None):\n    # Calculate accuracy\n    accuracy = accuracy_score(true_labels, predicted_probabilities.argmax(axis=1))\n\n    # Calculate top-5 accuracy\n    top5_accuracy = top_k_accuracy(true_labels, predicted_probabilities, k=5)\n\n    # Calculate F1 score\n    f1 = f1_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Calculate precision\n    precision = precision_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Calculate recall\n    recall = recall_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Plot ROC-AUC curve\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(len(class_names)):\n        fpr[i], tpr[i], _ = roc_curve(true_labels == i, predicted_probabilities[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Plot ROC-AUC curve\n    fig_roc = go.Figure()\n    for i in range(len(class_names)):\n        fig_roc.add_trace(go.Scatter(x=fpr[i], y=tpr[i], mode='lines', name=f'{class_names[i]} (AUC = {roc_auc[i]:0.2f})'))\n    fig_roc.update_layout(title='ROC Curve',\n                          xaxis_title='False Positive Rate',\n                          yaxis_title='True Positive Rate',\n                          width=1000,\n                          height=1000)\n    if save_path:\n        fig_roc.write_html(save_path)  # Save the plot to the specified path as HTML\n    fig_roc.show()\n\n    print(f'Accuracy: {accuracy}')\n    print(f'Top-5 Accuracy: {top5_accuracy}')\n    print(f'F1 Score: {f1}')\n    print(f'Precision: {precision}')\n    print(f'Recall: {recall}')\n\n# Predict labels and generate confusion matrix\ndef plot_confusion_matrix_and_metrics(generator, model, class_names, title, save_path=None):\n    # Get true labels and predicted labels\n    true_labels, predicted_labels, predicted_probabilities = get_true_and_predicted(generator, model)\n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n    plot_confusion_matrix(conf_matrix, class_names, title, save_path)\n\n    # Calculate metrics and plot ROC-AUC curve\n    calculate_metrics_and_plot_roc(np.array(true_labels), np.array(predicted_probabilities), class_names, title, save_path=None)\n\n# Function to calculate top-k accuracy\ndef top_k_accuracy(true_labels, predicted_probabilities, k=5):\n    top_k_predictions = np.argsort(predicted_probabilities, axis=1)[:, -k:]  # Get top k predictions\n    matches = np.zeros_like(true_labels)\n    for i in range(k):\n        matches += (top_k_predictions[:, i] == true_labels)\n    top_k_accuracy = np.mean(matches)\n    return top_k_accuracy\n\n\nclass_names = [k for k, v in dict(sorted(train_generator.class_indices.items(), key=lambda item: item[1])).items()]\n\n# Call above plot and metrics functions\nplot_confusion_matrix_and_metrics(train_generator, model, class_names, 'Confusion Matrix - Training Set', save_path=f\"{MODEL_SAVING_PATH}/training_set_plotly.html\")\nplot_confusion_matrix_and_metrics(validation_generator, model, class_names, 'Confusion Matrix - Validation Set', save_path=f\"{MODEL_SAVING_PATH}/validation_set_plotly.html\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to plot confusion matrix using Plotly\ndef plot_confusion_matrix(conf_matrix, class_names, title, save_path=None):\n    trace = go.Heatmap(z=np.flipud(conf_matrix),  # Reverse the y-axis\n                       x=class_names,\n                       y=class_names[::-1],  # Reverse the y-axis labels\n                       colorscale='Blues',\n                       colorbar=dict(title='Count'),\n                       )\n    layout = go.Layout(title=title,\n                       xaxis=dict(title='Predicted labels'),\n                       yaxis=dict(title='True labels'),\n                       width=500,  # Specify width of the plot\n                       height=500,  # Specify height of the plot\n#                        legend=dict(title='Legend'),  # Include legend\n                       )\n    fig = go.Figure(data=[trace], layout=layout)\n    if save_path:\n        fig.write_html(save_path)  # Save the plot to the specified path as HTML\n    fig.show()\n\n# Function to calculate metrics and plot ROC-AUC curve\ndef calculate_metrics_and_plot_roc(true_labels, predicted_probabilities, class_names, title, save_path=None):\n    # Calculate accuracy\n    accuracy = accuracy_score(true_labels, predicted_probabilities.argmax(axis=1))\n\n    # Calculate top-5 accuracy\n    top5_accuracy = top_k_accuracy(true_labels, predicted_probabilities, k=5)\n\n    # Calculate F1 score\n    f1 = f1_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Calculate precision\n    precision = precision_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Calculate recall\n    recall = recall_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Plot ROC-AUC curve\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i, v in enumerate(class_names):\n        fpr[i], tpr[i], _ = roc_curve(true_labels == v, predicted_probabilities[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Plot ROC-AUC curve\n    fig_roc = go.Figure()\n    for i in range(len(class_names)):\n        fig_roc.add_trace(go.Scatter(x=fpr[i], y=tpr[i], mode='lines', name=f'{class_names[i]} (AUC = {roc_auc[i]:0.2f})'))\n    fig_roc.update_layout(title='ROC Curve',\n                          xaxis_title='False Positive Rate',\n                          yaxis_title='True Positive Rate',\n                          width=750,\n                          height=750)\n    if save_path:\n        fig_roc.write_html(save_path)  # Save the plot to the specified path as HTML\n    fig_roc.show()\n\n    print(f'Accuracy: {accuracy}')\n    print(f'Top-5 Accuracy: {top5_accuracy}')\n    print(f'F1 Score: {f1}')\n    print(f'Precision: {precision}')\n    print(f'Recall: {recall}')\n\n# Predict labels and generate confusion matrix\ndef plot_confusion_matrix_and_metrics(generator, model, class_names, title, save_path=None):\n    # Get true labels and predicted labels\n    true_labels, predicted_labels, predicted_probabilities = get_true_and_predicted(generator, model)\n    print(f\"True labels: {true_labels}\")\n    print(f\"Predicted labels: {predicted_labels}\")\n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n    plot_confusion_matrix(conf_matrix, class_names, title, save_path)\n\n    # Calculate metrics and plot ROC-AUC curve\n    calculate_metrics_and_plot_roc(np.array(true_labels), np.array(predicted_probabilities), class_names, title, save_path=None)\n\n# Function to calculate top-k accuracy\ndef top_k_accuracy(true_labels, predicted_probabilities, k=5):\n    top_k_predictions = np.argsort(predicted_probabilities, axis=1)[:, -k:]  # Get top k predictions\n    matches = np.zeros_like(true_labels)\n    for i in range(k):\n        matches += (top_k_predictions[:, i] == true_labels)\n    top_k_accuracy = np.mean(matches)\n    return top_k_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model loading and evaluation on test","metadata":{}},{"cell_type":"code","source":"TEST_DATA_DIRECTORY = \"/kaggle/input/sneakers-dataset-test-split/TestDataset\"\n\nTEST_CLASS_MAPPING = {\n  '10010': 0,\n  '10011': 1,\n  '10024': 2,\n  '10050': 3,\n  '10053': 4,\n  '10031': 5,\n  '10067': 6,\n  '10068': 7,\n  '10003': 8,\n  '10004': 9,\n  '10013': 10,\n  '10014': 11,\n  '10015': 12,\n  '10016': 13,\n  '10017': 14,\n  '10018': 15,\n  '10023': 16,\n  '10046': 17,\n  '10047': 18,\n  '10001': 19,\n  '10002': 20,\n  '10005': 21,\n  '10006': 22,\n  '10008': 23,\n  '10019': 24,\n  '10020': 25,\n  '10039': 26,\n  '10041': 27,\n  '10077': 28,\n  '10091': 29,\n  '10092': 30,\n  '10104': 31,\n  '10105': 32\n}\n\n\n# Create train and validation data generators\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    TEST_DATA_DIRECTORY,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='training',  # specify this as training data\n    seed=SEED,\n    classes=TEST_CLASS_MAPPING\n)\n\n\nall_labels = set(test_generator.labels)\nnumber_of_labels = len(all_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generator.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reversed_test_mapping = {v: k for k, v in test_generator.class_indices.items()}\nreversed_test_mapping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_class_names = [reversed_test_mapping[test_label] for test_label in all_labels]\ntest_class_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_test_confusion_matrix(conf_matrix, class_names, title, save_path=None):\n    \n    trace = go.Heatmap(z=np.flipud(conf_matrix),  # Reverse the y-axis\n                       x=class_names,\n                       y=class_names[::-1],  # Reverse the y-axis labels\n                       colorscale='Blues',\n                       colorbar=dict(title='Count'),\n                       )\n    layout = go.Layout(title=title,\n                       xaxis=dict(title='Predicted labels'),\n                       yaxis=dict(title='True labels'),\n                       width=500,  # Specify width of the plot\n                       height=500,  # Specify height of the plot\n#                        legend=dict(title='Legend'),  # Include legend\n                       )\n    fig = go.Figure(data=[trace], layout=layout)\n    if save_path:\n        fig.write_html(save_path)  # Save the plot to the specified path as HTML\n    fig.show()\n    \n# Function to calculate metrics and plot ROC-AUC curve\ndef calculate_test_metrics_and_plot_roc(true_labels, predicted_probabilities, class_names, title, save_path=None):\n    # Calculate accuracy\n    accuracy = accuracy_score(true_labels, predicted_probabilities.argmax(axis=1))\n\n    # Calculate top-5 accuracy\n    top5_accuracy = top_k_accuracy(true_labels, predicted_probabilities, k=5)\n\n    # Calculate F1 score\n    f1 = f1_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Calculate precision\n    precision = precision_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Calculate recall\n    recall = recall_score(true_labels, predicted_probabilities.argmax(axis=1), average='macro')\n\n    # Plot ROC-AUC curve\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i, v in enumerate(class_names):\n        print(i, v)\n        i = test_generator.class_indices[v]\n        fpr[i], tpr[i], _ = roc_curve(true_labels == v, predicted_probabilities[:, i])\n#         print(predicted_probabilities[:, i])\n#         print(predicted_probabilities[:, test_generator.class_indices[v]])\n        print(fpr[i], tpr[i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n#         print(roc_auc[i])\n\n    # Plot ROC-AUC curve\n    fig_roc = go.Figure()\n    for i, v in enumerate(class_names):\n        i = test_generator.class_indices[v]\n        fig_roc.add_trace(go.Scatter(x=fpr[i], y=tpr[i], mode='lines', name=f'{v} (AUC = {roc_auc[i]:0.2f})'))\n    fig_roc.update_layout(title='ROC Curve',\n                          xaxis_title='False Positive Rate',\n                          yaxis_title='True Positive Rate',\n                          width=500,\n                          height=500)\n    if save_path:\n        fig_roc.write_html(save_path)  # Save the plot to the specified path as HTML\n    fig_roc.show()\n\n    print(f'Accuracy: {accuracy}')\n    print(f'Top-5 Accuracy: {top5_accuracy}')\n    print(f'F1 Score: {f1}')\n    print(f'Precision: {precision}')\n    print(f'Recall: {recall}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get true labels and predicted labels\ntest_true_labels, test_predicted_labels, test_predicted_probabilities = get_true_and_predicted(test_generator, model)\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(test_true_labels, test_predicted_labels)\n\nplot_test_confusion_matrix(conf_matrix, [\"10067\"] + test_class_names,  \"Confusion Matrix - Test Set\", save_path=f\"{MODEL_SAVING_PATH}/test_set_plotly.html\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Calculate metrics and plot ROC-AUC curve\n# calculate_test_metrics_and_plot_roc(np.array(test_true_labels), np.array(test_predicted_probabilities), [\"10067\"] + test_class_names, \"Test Set Roc Auc\", save_path=f\"{MODEL_SAVING_PATH}/test_set_roc_auc.html\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generator.class_indices.get('1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check workability of code that loads model weights","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom keras.models import load_model\n\nLOAD_MODEL_PATH = \"/kaggle/working/models/DATA_V2_<function EfficientNetB1 at 0x7bbeba678d30>_0Frozen_32Batch_240x240Size_AugV2/model.keras\"\nLOAD_MODEL_PATH_TO_USE = \"/kaggle/working/copied_model.keras\"\n\n# Copy the model to the destination\nshutil.copy(LOAD_MODEL_PATH, LOAD_MODEL_PATH_TO_USE)\n\n# Load the model from the destination path\n# downloaded_model = load_model(LOAD_MODEL_PATH_TO_USE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Below loading is incorrect","metadata":{}},{"cell_type":"code","source":"# number_of_labels = 33\n\n# def create_model():\n#     base_model = MODEL_CLASS_CALLBACK(input_tensor=input_layer, weights='imagenet', include_top=False)\n#     # # Freeze layers\n#     for layer in base_model.layers[:NUMBER_LAYERS_TO_FREEZE]:\n#         layer.trainable = False\n    \n#     x = base_model.output\n#     x = GlobalAveragePooling2D()(x)\n#     predictions = Dense(number_of_labels, activation='softmax')(x)\n#     model = Model(inputs=base_model.input, outputs=predictions)\n#     return model\n\n# downloaded_model = create_model()\n\n# # Compile the model\n# downloaded_model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=[\"categorical_accuracy\"])# metrics=[\"sparse_categorical_accuracy\"])\n# # downloaded_model = \n# downloaded_model.load_weights(\"/kaggle/working/models/DATA_V2_<function EfficientNetB1 at 0x7f46f2774d30>_0Frozen_32Batch_240x240Size_AugV2/model_weights_epoch_13.weights.h5\")\n\n# # Load the model from the destination path\n# # downloaded_model = load_model(LOAD_MODEL_PATH_TO_USE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# downloaded_model =  tf.keras.models.load_model(LOAD_MODEL_PATH)\n# downloaded_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Get true labels and predicted labels\n# test_true_labels, test_predicted_labels, test_predicted_probabilities = get_true_and_predicted(test_generator, downloaded_model)\n\n# # Generate confusion matrix\n# conf_matrix = confusion_matrix(test_true_labels, test_predicted_labels)\n\n# plot_test_confusion_matrix(conf_matrix, [\"10067\"] + test_class_names,  \"Confusion Matrix - Test Set\", save_path=f\"{MODEL_SAVING_PATH}/test_set_plotly.html\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_model_test():\n#     base_model = MODEL_CLASS_CALLBACK(input_tensor=input_layer, weights='imagenet', include_top=False)\n#     # # Freeze layers\n#     for layer in base_model.layers[:NUMBER_LAYERS_TO_FREEZE]:\n#         layer.trainable = False\n    \n#     x = base_model.output\n#     x = GlobalAveragePooling2D()(x)\n#     predictions = Dense(33, activation='softmax')(x)\n#     model = Model(inputs=base_model.input, outputs=predictions)\n#     return model\n\n# model_from_checkpoint = create_model_test()\n# checkpoint_path = \"/kaggle/working/models/DATA_V2_<function EfficientNetB1 at 0x7bbeba678d30>_0Frozen_32Batch_240x240Size_AugV2/model_weights_epoch_13.weights.h5\"\n# model_from_checkpoint.load_weights(checkpoint_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Get true labels and predicted labels\n# test_true_labels, test_predicted_labels, test_predicted_probabilities = get_true_and_predicted(test_generator, model_from_checkpoint)\n\n# # Generate confusion matrix\n# conf_matrix = confusion_matrix(test_true_labels, test_predicted_labels)\n\n# plot_test_confusion_matrix(conf_matrix, [\"10067\"] + test_class_names,  \"Confusion Matrix - Test Set\", save_path=f\"{MODEL_SAVING_PATH}/test_set_plotly.html\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save('my_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = tf.keras.models.load_model('my_model.h5')\n# new_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get true labels and predicted labels\ntest_true_labels, test_predicted_labels, test_predicted_probabilities = get_true_and_predicted(test_generator, new_model)\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(test_true_labels, test_predicted_labels)\n\nplot_test_confusion_matrix(conf_matrix, [\"10067\"] + test_class_names,  \"Confusion Matrix - Test Set\", save_path=f\"{MODEL_SAVING_PATH}/test_set_plotly.html\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\n\n\nWe loaded weights with `model.save(\"model.keras\")` and read it with `tf.keras.models.load_model` and evaluation made it clear that model weights has not been loaded.\n\n\nAfter reading a couple of articles we decided to save model in `.h5` format and it worked just fine.","metadata":{}},{"cell_type":"markdown","source":"Kudos to:\n\nhttps://www.kaggle.com/code/vikramtiwari/tf2-tutorials-keras-save-and-restore-models","metadata":{}},{"cell_type":"markdown","source":"  ","metadata":{}}]}